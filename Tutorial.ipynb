{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incredible Deep Learning in immunooncology with Pytorch\n",
    "\n",
    "This tutorial requires some pre-requisites: basic Python and preferably Machine Learning knowledge, experience with Anaconda and Jupyter Lab. We provide useful links to quickly learn those, if you are not yet familiar with them:\n",
    "\n",
    "- A neat introduction to python - https://www.w3schools.com/python/default.asp\n",
    "\n",
    "- Introduction to Anaconda - an essential package manager for Python - https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html\n",
    "\n",
    "- Introduction to Jupyter Lab - an intergrated development environment for writing Python code and data analysis. Jupyter Lab is based on Jupyter Notebook philosophy and runs Jupyter Notebooks, you can learn about them here - https://www.dataquest.io/blog/jupyter-notebook-tutorial/. If you want a Jupyter Lab tutorial, here is a nice video explaining all the basics - https://www.youtube.com/watch?v=NXwP8pSOiB8.\n",
    "\n",
    "A nice intro to Deep Learning\n",
    "https://towardsdatascience.com/an-introduction-to-deep-learning-af63448c122c\n",
    "\n",
    "PyTorch intro\n",
    "https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "Building pMHCI classifier\n",
    "http://dmnfarrell.github.io/bioinformatics/mhclearning\n",
    "\n",
    "Building pMHCI classifier 2\n",
    "https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy/\n",
    "\n",
    "A PyTorch tutorial\n",
    "https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "- MHC and cancer immunotherapy\n",
    "- Quick intro to Machine Learning\n",
    "- Predicting peptide-MHCI binders\n",
    "- Further directions and ideas to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHC and cancer immunotherapy\n",
    "\n",
    "![MHC I class complex](assets/mhci.jpg)\n",
    "\n",
    "MHC stands for major histocompatibility complex. It is an ensemble of proteins on the cell surface that present peptides to TCR receptor and determines histocompatibility through this interaction. All human body cells that have nucleus usually present cytosolic peptides of their own origin, derived from the cell metabolism and defective ribosomal products. \n",
    "\n",
    "In a cell, protein molecules of the host's own phenotype or of other biologic entities are continually synthesized and degraded. During intracellular infection or cell transformation foreign or mutated proteins also degrade and get presented on sell surface within the peptide-binding groove of the MHC molecule. Interestingly enough, T-cells can detect peptides presented on as little as 0.1%-1% MHC complexes. \n",
    "\n",
    "These peptides can be studied in the laboratory with HTS peptidome sequencing techniques including mass-spectrometry and various biochemical methods. One way to perform it, is to acquire cell lines expressing single MHC allele or in vitro purified MHC complex. After addition of a mixture of peptides to the system, it should be purified and the binders should be sequenced. \n",
    "\n",
    "TCR is responsible for recognition of both MHC complex and peptide epitope presented. If a cell does not bear MHC on its surface it cannot be detected by cytotoxic T-cell. Some cancers develop this strategy to avoid immune system inspection and consequent immune response. However these cells normally must be eliminated by natural killers (NK) and NKT subsets of lymphocytes. Thus, immune system is able do kill both cells that present foreign peptide and cells that lose their MHC due to mutation or another event, probably in the process of cancer transformation. \n",
    "\n",
    "This mechanism is very efficient in terms of infection and cell malignisation control. However the same mechanism poses several medical challenges. It comes with no surprise that the recipient-donor pair matching for biological materials and organ transplantation is one of the most difficult problems. It stems from the frequent transplant rejections caused by antigen mismatch that causes host-versus-graft disease. Sometimes graft-versus-host disease follows the receipt of transplanted tissue from a genetically different person. For instance, in the case of bone marrow transplantation the graft's lymphocytes may attack body cells recognised as foreign.\n",
    "\n",
    "\n",
    "Peptide binding to MHC molecules is the key selection step in the Antigen-presentation pathway. This is essential for T cell immune responses. The ‘epitope’ is the peptide-MHC combination shown in the image at right. Key residues in the MHC contact the peptide and these differ between alleles. The prediction of peptide binding to MHC molecules has been much studied. The problem is simpler for class-I molecules since the binding peptide length is less variable (usually 8-11 but commonly 9). Typically binding predictors are based on training models with experimental binding affinity measurements with known peptide sequences. This data is available from the IEDB for many human alleles. New peptides can then be predicted based on their position specific similarity to the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adoptive T-cell immunotherapy\n",
    "\n",
    "Adoptive T-cell therapy is a form of cancer immunotherapy that aims to isolate tumor infiltrating T-cells from the tumor in the patient, possibly genetically engineer them to be cancer-specific, grow them in great numbers and reintroduce them into the body to fight the cancer. In order to terminate cancer cells, the T-cell needs to be activated by being exposed to tumor peptides bound to MHCI (pMHCI). By analyzing the tumor genetics, relevant peptides can be identified and depending on the patients particular type of MHCI, we can predict which pMHCI are likely to be present in the tumor in the patient and thus which pMHCIs should be used to activate the T-cells.\n",
    "\n",
    "From [here](https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick intro to Machine Learning\n",
    "\n",
    "Machine learning is all about algorithms or \"machines\" that learn *something* from the input data that helps you solve your task. But what task are you solving? There are different types of them, which can be combined in a number of high-level descriptions. We will focus on the one type of tasks - a classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification task\n",
    "\n",
    "Classification task is type of task where the algorithm predicts the label or class based on unlabeled data. The algorithm has to be trained first on a set of known objects and their labels. For example, a set may contain the pictures of pets, where a picture of a dog is an object, and \"dog\" is its label. In our case, we have peptides as objects and labels 'True\" or \"False\" if they bind or do not bind the MHC class I respectively.\n",
    "\n",
    "The typical workflow has four steps: \n",
    "\n",
    "- input data - the object-label pairs for model training\n",
    "\n",
    "- create model - the exact algorithm that will predict us binders or dogs in the first example\n",
    "\n",
    "- choose loss function - a function to evaluate how well our algorithm predicts the label\n",
    "\n",
    "- choose optimisation algorithm - an algorithm that \"guides\" our model so it learns how to predict the right label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the training data and one-hot encoding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# upload data here\n",
    "train_data = pd.read_csv(\"data/train.csv.gz\")\n",
    "print(\"Dataset size:\", len(train_data), \"peptides\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For educational purposes we will be training the model only on just one allele. For this reason we will filter the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count alleles\n",
    "print(Counter(train_data[\"mhc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count alleles\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.countplot(y=train_data[\"mhc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all non-human alleles\n",
    "train_data = train_data.loc[train_data[\"mhc\"].str.startswith(\"HLA\"), ]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.countplot(y=train_data[\"mhc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: filter out non HLAA0201 alleles\n",
    "\n",
    "# your code is here\n",
    "\n",
    "print(\"Filtered dataset size:\", len(train_data), \"peptides\")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.countplot(y=train_data[\"mhc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: filter out non 9-mers\n",
    "# You will need https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.len.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and drop duplicates\n",
    "train_size_pre = len(train_data)\n",
    "train_data.drop_duplicates([\"mhc\", \"sequence\"], inplace=True)\n",
    "print(\"Dropped\", train_size_pre - len(train_data), \"duplicates\")\n",
    "print(\"Deduplicated dataset:\", len(train_data), \"peptides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: follow the same pipeline for the testing data\n",
    "# Suggestion: try not to look very often to the previous code\n",
    "\n",
    "# load data\n",
    "# print a number of peptides in the data\n",
    "# plot a distribution of alleles\n",
    "# filter out all non HLAA0201 alleles\n",
    "# filter out all non 9-mers\n",
    "# remove duplicates\n",
    "# how many duplicates did you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any common peptides and drop them as well\n",
    "\n",
    "test_data = pd.read_csv(\"data/test.csv.gz\")\n",
    "\n",
    "pd.merge(train_data, test_data, \"inner\", [\"mhc\", \"sequence\"]) # and there is none!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert IC50 values\n",
    "\n",
    "print(train_data[\"meas\"].max())\n",
    "train_data.loc[train_data[\"meas\"] > 50000, \"meas\"] = 50000\n",
    "print(train_data[\"meas\"].max())\n",
    "train_data[\"meas\"] = 1 - np.log(train_data[\"meas\"]) / np.log(50000)\n",
    "\n",
    "# Task: do the same for test data\n",
    "test_data = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's why you need to learn how to make functions :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create one-hot encoding\n",
    "\n",
    "# First, create a dictionary aminoacid -> feature vector (one-hot vector)\n",
    "def create_dict(dataframe):\n",
    "    d = { x:y for y,x in enumerate(sorted(set(dataframe[\"sequence\"].str.cat()))) }\n",
    "    for i, aa in enumerate(d):\n",
    "        vec = torch.zeros((len(d), ))\n",
    "        vec[i] = 1\n",
    "        d[aa] = vec\n",
    "    return d\n",
    "\n",
    "onehot_dict = create_dict(train_data)\n",
    "onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def make_onehot(dataframe, feature_dict):\n",
    "    max_peptide_size = dataframe[\"sequence\"].str.len().max()\n",
    "    res = torch.zeros((len(dataframe), max_peptide_size, len(feature_dict))) # 3d tensor (X, Y, Z)\n",
    "    for i, pep in tqdm(enumerate(dataframe[\"sequence\"]), total=len(dataframe[\"sequence\"])):\n",
    "        for j, aa in enumerate(pep):\n",
    "            res[i][j] = feature_dict[aa]\n",
    "    return res\n",
    "\n",
    "X_train = make_onehot(train_data, onehot_dict)\n",
    "# X_test = make_onehot(test_data, onehot_dict)\n",
    "print(X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Machine Learning models require feature engineering, i.e., the manual selection or creation of features using statistics, mathematical operations, etc. The Neural Networks are user-friendly and can process all features without any engineering, although the inner representation of data remains a black box for the data scientist.\n",
    "\n",
    "Due to this fact, it is difficult to track the influence of individual features on the overall result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "\n",
    "Now as we have finished with the data preprocessing and have got a nice small dataset to train on, we need a model. A mathemathical model, called \"classifier\", is a model that takes input X and uses it to predict an output.\n",
    "\n",
    "Generally speaking, deep learning is a machine learning method that takes in an input train data, and uses it to predict an output of test data.\n",
    "\n",
    "We will use PyTorch - a powerful framework for Deep Learning that most researchers in the world love. There are other frameworks, such as TensorFlow that is more popular than PyTorch (or at least what people say...), but PyTorch is better in our opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/720/0*4aHRjVXRKsyUhm2b\" width=\"600\" />\n",
    "\n",
    "\n",
    "A neural network is composed of input, hidden, and output layers — all of which are composed of “nodes” or \"computational units\" (previously referred as \"neurons\"). Input layers take in a numerical representation of data (e.g. images with pixel specs), output layers output predictions, while hidden layers are correlated with most of the computation.\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/720/0*C_nYHONjxjUVgIYh\" width=\"600\" />\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/720/0*5kA2vn4UqmI4iKLM\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of the model above done in PyTorch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Sequential, Linear, Sigmoid\n",
    "\n",
    "model = nn.Sequential(Linear(3, 4), # input layer -> hidden layer 1\n",
    "                      Sigmoid(), \n",
    "                      Linear(4, 4), # hidden layer 1 -> hidden layer 2\n",
    "                      Sigmoid(),\n",
    "                      Linear(4, 1), # hidden layer 2 -> output layer\n",
    "                      Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "But how model will understand, how to use the input data to learn from it? \n",
    "Given a large dataset of input and output pairs, a deep learning algorithm will try to minimize the difference between its prediction and expected output. By doing this, it tries to learn the association/pattern between given inputs and outputs — this in turn allows a deep learning model to generalize to inputs that it hasn’t seen before.\n",
    "\n",
    "We will minimize the Binary Cross-Entropy loss.\n",
    "\n",
    "In order to use BCE loss with Pytorch you just need to execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "\n",
    "criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation algorithm\n",
    "\n",
    "batch learning\n",
    "picture with local minima\n",
    "speed of optimization (learning rate)\n",
    "nn.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting peptide-MHCI binders\n",
    "\n",
    "And finally we can integrate everything we learned so far into the main training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm for training neural networks:\n",
    "\n",
    "  1. Check: overfit one batch - both train and test to test the model and functions\n",
    "  2. Overfit: make the best overfitting model as possible\n",
    "  3. Regularise: try to add regularizers to increase the validation score\n",
    "\n",
    "For details see this blogpost: http://karpathy.github.io/2019/04/25/recipe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "THRESHOLD = 0.426\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "y = torch.from_numpy(train_data[\"meas\"].values).reshape((-1,1)).float()\n",
    "y[y >= THRESHOLD] = 1\n",
    "y[y < THRESHOLD] = 0\n",
    "\n",
    "dataset = TensorDataset(X_train.view((X_train.shape[0], -1)), y)\n",
    "dl = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(Linear(300, 64),\n",
    "                      Sigmoid(), \n",
    "                      Linear(64, 1),\n",
    "                      Sigmoid())\n",
    "criterion = BCELoss()\n",
    "opt = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    stats = {\"loss\": [], \"acc\": []}\n",
    "    for X, y_true in dl:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        y_pred[y_pred >= THRESHOLD] = 1\n",
    "        y_pred[y_pred < THRESHOLD] = 0\n",
    "        stats[\"loss\"].append(loss.item())\n",
    "        stats[\"acc\"].append(y_true.long().eq(y_pred.long()).sum() / X.shape[0])\n",
    "    \n",
    "    print(\"Epoch:\", i_epoch, \"Loss:\", np.mean(stats[\"loss\"]), \"Acc:\", np.mean(stats[\"acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        stats = {\"loss\": [], \"acc\": []}\n",
    "        for X, y_true in dl:\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y_true)\n",
    "\n",
    "            y_pred[y_pred >= THRESHOLD] = 1\n",
    "            y_pred[y_pred < THRESHOLD] = 0\n",
    "            stats[\"loss\"].append(loss.item())\n",
    "            stats[\"acc\"].append(y_true.long().eq(y_pred.long()).sum() / X.shape[0])\n",
    "    \n",
    "        print(\"Test:\", i_epoch, \"Loss:\", np.mean(stats[\"loss\"]), \"Acc:\", np.mean(stats[\"acc\"]))\n",
    "            \n",
    "    model.train()\n",
    "    \n",
    "test(model, criterion, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting ideas to explore\n",
    "\n",
    "## Per-allele model\n",
    "\n",
    "### 1. Find the best encoding\n",
    "\n",
    "There are numerous ways to encode amino acids. The most notable are BLOSUM, Kidera and Athley factors. You can find some of them in our [repository here](https://github.com/vadimnazarov/kidera-atchley). \n",
    "\n",
    "### 2. Use embeddings\n",
    "\n",
    "Word embedding is a vector representation of a word, encoding its meaning in some context. \n",
    "\n",
    "[Here](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/) is a quick and good introduction to word embeddings. And [here](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) you can find a little bit harder PyTorch tutorial on word embeddings with PyTorch code needed for infer your own representation of amino acids.\n",
    "\n",
    "### 3. Use different training subroutines\n",
    "\n",
    "Changing learning rate, balancing batches or make it a regression task instead of classification may help.\n",
    "\n",
    "### 4. Use convolutional networks to extract binding motifs\n",
    "\n",
    "### 5. Use recurrent neural networks\n",
    "\n",
    "#### 5.1. Use RNNs with Attention and extract binding motifs\n",
    "\n",
    "### 6. Think about scaling: create a smallest and fastest possible model for quick analysis of 10,000,000 peptides\n",
    "\n",
    "### 7. Create a binder space: use autoencoders, variational autoencoders or generative adversarial networks\n",
    "\n",
    "## Pan-allele model\n",
    "\n",
    "Pan-allele models such as [NetMHCpan](http://www.cbs.dtu.dk/services/NetMHCpan/) are models that uses information about both peptides and alleles. A model analyses both peptide and MHC allele sequences and can be used for prediction of previously unknown MHC alleles. Additional advantage of such types of models is that by integrating information about both peptide and MHC sequences the resulting model shows the higher accuracy per allele in contrast to per-allele models.\n",
    "\n",
    "Ideas to explore here is pretty much the same as in the **Per-allele** part, but with a nice yet complex addition of MHC amino acid sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
